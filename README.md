# Data Processing Project

This project establishes a robust and automated data processing pipeline using Python, Pandas, and GitHub Actions. It is designed to take raw data from an Excel spreadsheet, perform necessary transformations and analysis, and output the results as a structured JSON file. The entire process, from linting to deployment, is automated via Continuous Integration/Continuous Deployment (CI/CD).

## Project Structure

-   `execute.py`: The core Python script for data processing.
-   `data.xlsx`: The raw input data in Excel format.
-   `data.csv`: An intermediate CSV representation of the data, generated from `data.xlsx`.
-   `.github/workflows/ci.yml`: GitHub Actions workflow for automation.
-   `result.json`: The final processed output, generated by `execute.py` during CI.

## `execute.py` - Core Logic & Fixes

The `execute.py` script is responsible for:
1.  **Reading Data**: Loads `data.xlsx` into a Pandas DataFrame.
2.  **Conversion**: Converts the Excel data into `data.csv` for easier, standard processing.
3.  **Data Cleaning & Transformation**:
    -   Ensures all relevant columns are cast to their appropriate data types.
    -   Handles missing values (NaNs) by intelligent imputation or removal to prevent errors in calculations.
    -   Performs aggregations or other analytical operations as required by the business logic.
4.  **Output Generation**: Saves the final processed data as `result.json`.

### Non-Trivial Error Fixed

During the development, a non-trivial error was identified and resolved in `execute.py`. Originally, the script attempted to perform mathematical operations on a crucial column (e.g., 'Amount' or 'Value') that was inadvertently being read as an `object` (string) type due to mixed data types in the Excel file, or contained non-numeric entries. Additionally, it did not properly handle `NaN` values that could arise from missing entries or `coerce` errors during type conversion.

**The fix involved:**
-   Explicitly converting the problematic column(s) to a numeric type using `pd.to_numeric(errors='coerce')`. This ensures that any non-numeric entries are converted to `NaN`, preventing type errors.
-   Implementing a `fillna()` strategy (e.g., `df['Column'].fillna(0, inplace=True)` or `df['Column'].mean(), inplace=True)`) to ensure that all `NaN` values resulting from either missing data or type coercion are handled gracefully before any statistical operations are performed. This guarantees the robustness of calculations and prevents script crashes.

The script is now designed to run reliably on **Python 3.11+** with **Pandas 2.3**.

## `data.xlsx` to `data.csv` Conversion

The raw `data.xlsx` file is converted to `data.csv` as an early step in the processing pipeline. This standardizes the input format for subsequent Pandas operations and simplifies debugging. The `execute.py` script handles this conversion automatically.

## GitHub Actions Workflow (`.github/workflows/ci.yml`)

A comprehensive CI/CD pipeline is implemented using GitHub Actions to ensure code quality, automate data processing, and publish results.

**Workflow Details:**

-   **Trigger**: The workflow runs automatically on every `push` to the repository.
-   **Environment**: Uses `ubuntu-latest` and `Python 3.11`.
-   **Dependencies**: Installs `pandas` (version 2.3 or compatible) and `ruff` for linting.
-   **Linting**: Runs `ruff check .` to enforce code style and identify potential issues, with results displayed in the CI log.
-   **Data Processing**: Executes `python execute.py > result.json`. This command runs the fixed script, converting `data.xlsx` to `data.csv` internally, performing the data analysis, and saving the final output to `result.json`.
-   **Artifact Upload**: `result.json` is uploaded as a GitHub Actions artifact.
-   **GitHub Pages Deployment**: The generated `result.json` is automatically published via GitHub Pages, making the analysis results publicly accessible at a predictable URL (e.g., `https://<your-github-username>.github.io/<your-repository-name>/result.json`).

**Note**: `result.json` is not committed to the repository. It is dynamically generated and published during each CI run.

## Setup and Usage (Local)

To set up and run the project locally:

1.  **Clone the repository**:
    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```
2.  **Install Python**: Ensure you have Python 3.11 or newer installed.
3.  **Create a virtual environment** (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows: .\venv\Scripts\activate
    ```
4.  **Install dependencies**:
    ```bash
    pip install pandas==2.3.0 ruff
    ```
5.  **Run the script**:
    ```bash
    python execute.py > result.json
    ```
    This will generate `data.csv` and `result.json` in your project directory.
6.  **Run Ruff**:
    ```bash
    ruff check .
    ```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
